{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxGZ0ULSk00a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ResumeFlow â€” Databricks HR Resume Extraction Pipeline (Documentation Version)\n",
        "\n",
        "This script extracts structured candidate data from PDF resumes stored in a GitHub\n",
        "repository using a Databricks LLM endpoint, and writes them into a Delta table\n",
        "for analytics and dashboarding.\n",
        "\n",
        "Instructions:\n",
        "1. Fill in the placeholders below (Databricks endpoint, GitHub repo info, etc.)\n",
        "2. Run inside a Databricks notebook or job.\n",
        "3. Verify that the Delta table 'hr.resume.candidates_extracted_new' is created.\n",
        "\"\"\"\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Install dependencies (only needed once per cluster)\n",
        "# -------------------------------------------------------------\n",
        "# %pip install pdfminer.six pydantic openai requests pandas\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Imports\n",
        "# -------------------------------------------------------------\n",
        "import json\n",
        "import requests\n",
        "import io\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, List\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "try:\n",
        "    from pyspark.sql import SparkSession, functions as F\n",
        "    from pyspark.sql.types import DoubleType\n",
        "except ImportError:\n",
        "    print(\"Running in local mode; Spark not available.\")\n",
        "\n",
        "from pdfminer.high_level import extract_text_to_fp\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Databricks LLM Configuration\n",
        "# -------------------------------------------------------------\n",
        "GPT_MODEL_NAME = \"<your-databricks-llm-endpoint-model-name>\"\n",
        "GPT_BASE_URL = \"<your-databricks-workspace-url>/serving-endpoints\"\n",
        "\n",
        "try:\n",
        "    DATABRICKS_TOKEN = (\n",
        "        dbutils.notebook.entry_point.getDbutils()\n",
        "        .notebook()\n",
        "        .getContext()\n",
        "        .apiToken()\n",
        "        .get()\n",
        "    )\n",
        "    print(\"Using Databricks notebook context token.\")\n",
        "except Exception:\n",
        "    DATABRICKS_TOKEN = os.environ.get(\"DATABRICKS_TOKEN\", None)\n",
        "\n",
        "if not DATABRICKS_TOKEN:\n",
        "    raise ValueError(\n",
        "        \"No Databricks token found. Set DATABRICKS_TOKEN or run inside Databricks.\"\n",
        "    )\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=DATABRICKS_TOKEN, base_url=GPT_BASE_URL, timeout=60.0)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# GitHub Source Configuration\n",
        "# -------------------------------------------------------------\n",
        "GITHUB_USER = \"<your-github-username>\"\n",
        "GITHUB_REPO = \"<your-github-repo>\"\n",
        "GITHUB_BRANCH = \"<branch-name>\"\n",
        "GITHUB_FOLDER = \"<folder-containing-pdfs>\"\n",
        "\n",
        "GITHUB_BASE_URL = (\n",
        "    f\"https://raw.githubusercontent.com/{GITHUB_USER}/{GITHUB_REPO}/\"\n",
        "    f\"{GITHUB_BRANCH}/{GITHUB_FOLDER}/\"\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Data Schema\n",
        "# -------------------------------------------------------------\n",
        "class CandidateData(BaseModel):\n",
        "    fullName: str = Field(description=\"Name of the applicant.\")\n",
        "    email: str = Field(description=\"Email address.\")\n",
        "    phoneNumber: str = Field(description=\"Phone number.\")\n",
        "    educationDegree: str = Field(description=\"Highest educational degree.\")\n",
        "    currentCompany: str = Field(description=\"Current employer or most recent.\")\n",
        "    experienceYears: float = Field(description=\"Years of experience (float).\")\n",
        "    skillsText: str = Field(description=\"Comma-separated skills.\")\n",
        "    languagesText: str = Field(description=\"Comma-separated languages.\")\n",
        "    category: str = Field(description=\"Job category or field.\")\n",
        "\n",
        "\n",
        "TARGET_KEYS = list(CandidateData.model_fields.keys())\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# GitHub File Discovery\n",
        "# -------------------------------------------------------------\n",
        "def get_github_folder_contents(user: str, repo: str, branch: str, folder: str) -> List[str]:\n",
        "    \"\"\"Return a list of PDF file names from a GitHub repository folder.\"\"\"\n",
        "    api_url = f\"https://api.github.com/repos/{user}/{repo}/contents/{folder}?ref={branch}\"\n",
        "    print(f\"Fetching GitHub folder contents from: {api_url}\")\n",
        "    try:\n",
        "        response = requests.get(api_url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        contents = response.json()\n",
        "        return [\n",
        "            item[\"name\"] for item in contents if item.get(\"name\", \"\").lower().endswith(\".pdf\")\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching GitHub contents: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# PDF Text Extraction\n",
        "# -------------------------------------------------------------\n",
        "def pdf_to_text(file_url: str) -> str:\n",
        "    \"\"\"Download a PDF file and extract its text.\"\"\"\n",
        "    try:\n",
        "        r = requests.get(file_url, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        pdf_file = io.BytesIO(r.content)\n",
        "        output_string = io.StringIO()\n",
        "        extract_text_to_fp(pdf_file, output_string)\n",
        "        return output_string.getvalue().strip()\n",
        "    except Exception as e:\n",
        "        print(f\"PDF extraction error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# LLM Extraction Function\n",
        "# -------------------------------------------------------------\n",
        "def analyze_resume_with_llm(filename: str, file_url: str) -> Dict[str, Any]:\n",
        "    \"\"\"Use the Databricks LLM to extract structured candidate data from a resume.\"\"\"\n",
        "    pdf_text = pdf_to_text(file_url)\n",
        "    if not pdf_text:\n",
        "        return {}\n",
        "\n",
        "    if len(pdf_text) > 16000:\n",
        "        pdf_text = pdf_text[:16000]\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are an expert HR data extractor. Extract structured data from the provided resume text. \"\n",
        "        \"Output each field in the format key:value, one per line. \"\n",
        "        \"If a field is missing, leave it blank. The current year is 2025.\"\n",
        "    )\n",
        "\n",
        "    fields_list = \"\\n\".join(TARGET_KEYS)\n",
        "    user_prompt = f\"Extract the following fields:\\n{fields_list}\\n\\nResume Text:\\n{pdf_text}\"\n",
        "\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=GPT_MODEL_NAME,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt},\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "        )\n",
        "        raw_output = completion.choices[0].message.content\n",
        "        lines = [line for line in raw_output.splitlines() if \":\" in line]\n",
        "        data = {}\n",
        "        for line in lines:\n",
        "            key, val = line.split(\":\", 1)\n",
        "            key, val = key.strip(), val.strip()\n",
        "            if key in TARGET_KEYS:\n",
        "                data[key] = val\n",
        "        for k in TARGET_KEYS:\n",
        "            data.setdefault(k, \"\")\n",
        "        if data.get(\"experienceYears\"):\n",
        "            try:\n",
        "                data[\"experienceYears\"] = float(data[\"experienceYears\"])\n",
        "            except ValueError:\n",
        "                data[\"experienceYears\"] = 0.0\n",
        "        return CandidateData(**data).model_dump()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM extraction: {e}\")\n",
        "        return {}\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Batch Processing Logic\n",
        "# -------------------------------------------------------------\n",
        "def main():\n",
        "    \"\"\"Run the resume extraction pipeline.\"\"\"\n",
        "    files = get_github_folder_contents(GITHUB_USER, GITHUB_REPO, GITHUB_BRANCH, GITHUB_FOLDER)\n",
        "    if not files:\n",
        "        print(\"No PDF files found.\")\n",
        "        return\n",
        "\n",
        "    all_records = []\n",
        "    for f in files:\n",
        "        url = f\"{GITHUB_BASE_URL}{f}\"\n",
        "        print(f\"Processing: {f}\")\n",
        "        record = analyze_resume_with_llm(f, url)\n",
        "        if record:\n",
        "            record[\"sourceFile\"] = f\n",
        "            record[\"loadTs\"] = datetime.utcnow().isoformat()\n",
        "            all_records.append(record)\n",
        "\n",
        "    if not all_records:\n",
        "        print(\"No records extracted.\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(all_records)\n",
        "    print(f\"Extracted {len(df)} records.\")\n",
        "\n",
        "    try:\n",
        "        spark_df = spark.createDataFrame(df)\n",
        "        spark_df = spark_df.withColumn(\"experienceYears\", F.col(\"experienceYears\").cast(DoubleType()))\n",
        "        spark_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"hr.resume.candidates_extracted_new\")\n",
        "        print(\"Data saved to Delta table: hr.resume.candidates_extracted_new\")\n",
        "        display(spark_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not save to Delta. ({e})\")\n",
        "        display(df)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Run\n",
        "# -------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}